{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Implementation of Hardware Neural Networks Powered by Verython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "For our ECE 5760 final project, we implemented a convolutional neural network (CNN) on the FPGA to classify handwritten digits.  CNNs are among the most popular neural network architectures for image classification, as they have been shown to outperform humans in clinical imaging [(Source)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6586983/), and have a variety of recognition applications!  Interestingly, CNNs behave like a black box: they take in input and generate an output without revealing their intrinsic logic.  The goal of this project was to demystify CNNs by breaking their layers down into constituent components, and implementing them in hardware.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Design\n",
    "\n",
    "\n",
    "The term “convolutional” is derived from the mathematical operation of convolution - which conventionally involves multiplying elements together then summing them together.  The output of a convolution is therefore a description of how the shape of one function influences another function.  This unique property of our convolution is what our model captures: the spatial dependencies of an image.  For our digit classification task, a conventional neural network would be overfit to the specific location of the digit on the screen whereas the convolutional layer would learn the edges and gradients of the image. \n",
    "\n",
    "The architecture of our network is as follows: a series of convolutional layers with max pooling (twice), followed by a flattened, fully connected layer.  Below is the tensorflow (python library for building neural networks) summary of our model, highlighting all the parameters that we trained.  \n",
    "\n",
    "![image info](./images/tf.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we describe the high-level operations of each layer.\n",
    "\n",
    "Our convolutional layer uses a kernel size of 2x2.  In this example, we output a 3x3 feature map from a 4x4 image.  Our 2x2 filter slides across the image from left to right, accumulating the element-wise product summation.  We can extrapolate this result to NxN images with 2x2 kernels → the output feature map will always be (N-1) x (N-1) given that the kernel is sized 2x2.  \n",
    "\n",
    "\n",
    "![image info](./images/conv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a finishing touch to the output of our convolutional layer, we pass our values through a special function to “activate” or realize our predictions.  Each element of the feature map gets passed through an activation function before moving onto the next layer.  We chose the sigmoid activation function initially to keep our values between [0,1].  Extremely large positive values get normalized closer to 1 while large negative values get normalized closer to 0.  We soon realized we needed a large number of decimal bits in our fixed point representation.  We ended up resorting to ReLU activation, as the computational cost was cheaper and did not require significant decimal precision to activate values. In this activation function, negative values become zero while positive values retain their original value.\n",
    "\n",
    "![image info](./images/sigmoid.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately following the convolutional layer is our max-pooling layer.  Let’s take a 4x4 feature map, as an example.  Here, we use a pool size of 2x2 with a stride of 2.  At each stride, we take the maximum element of the 2x2 window.  This results in an output image of size 2x2. When the pool size is equal to the stride size, we effectively reduce the dimensions of the feature map by a factor of 2.  \n",
    "\n",
    "\n",
    "![image info](./images/mp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, our fully-connected layer is the flattened output of the feature map.  We connect it to our predicted digits of 0-9 and train our model to learn the weights of this layer.  The weights can be thought of as the strength of the connection: how much does the output depend on that specific connection?  We take the dot product of the flattened feature map with each of their respective weights and pass them through another activation function to generate our predictions.  We take the index of the element with the maximum probability in our 10 element vector array as our predictions for a given input.\n",
    "\n",
    "![image info](./images/fullyconnect.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Notation\n",
    "For some array $A$, let $A:[d1,\\dots,d_i,\\dots,d_n]$ indicate that that $A$ is $n$-dimensional and each dimension $1\\leq i\\leq n$ has a length of $d_i$. For example,\n",
    "$$A:[2, 3]=\\begin{bmatrix} a_{1, 1} & a_{1,2} & a_{1,3} \\\\ a_{2,1} & a_{2,2} & a_{2,3} \\end{bmatrix}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program/Hardware Design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "In this section, we will dive deep into the math behind each of the layers in the model and how we rederived the transformation functions of each layer to fit on the FPGA. \n",
    "\n",
    "As a reminder, our model ingests some image of a hand-drawn digit and outputs a number between 0 and 9 which corresponds to the prediction of the model. The image is a 28 by 28 2D array of values between 0 and 255, or rather, some input image $img$ can be represented as\n",
    "$$img=\\begin{bmatrix} \n",
    "  p_{1,1} & \\dots & p_{1,j} & \\dots & p_{1,28} \\\\ \n",
    "  \\vdots & \\ddots & & & \\vdots \\\\\n",
    "  p_{i, 1} & \\dots & p_{i, j} & \\dots & p_{i, 28} \\\\\n",
    "  \\vdots & & & \\ddots & \\vdots \\\\\n",
    "  p_{28, 1} & \\dots & p_{28, j} & \\dots & p_{28, 28}\n",
    "\\end{bmatrix}$$\n",
    "such that $0\\leq p_{i,j}\\leq255$. \n",
    "\n",
    "Moreover, we can think of our model $M$ as a function that maps some input $img$ to some output prediction $pred$, or rather, the inference of our model can be represented by \n",
    "$$M(img)=pred$$\n",
    "for some output prediction $0\\leq pred\\leq9$. \n",
    "\n",
    "Furtheremore, we can also express each of the layers in our model as a a function. Let $L=\\{L_1,\\dots,L_i,\\dots,L_4\\} be the layers in our model. Then we can represent each layer $L_i$ as some function \n",
    "$$L_i(V_i^{in})=V_i^{out},$$\n",
    "where $V_{in}$ and $V_{out}$ are matrices whose shapes are defined by $L_i$. \n",
    "\n",
    "With this new notation, we can redfine $M$ in terms of its layers:\n",
    "$$\\begin{align*}\n",
    "  M(img) & =L_4(L_3(L_2(L_1(img)))) \\\\\n",
    "  & =pred.\n",
    "\\end{align*}$$\n",
    "Thus, a model's prediction is simply the output of its cascaded layer functions.\n",
    "\n",
    "Let's now take a look at each of the layers and how we can rederive their functions to be built on the FPGA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1: Conv2D\n",
    "The 2D convolutional layer has weights $W^1$ and biases $B^1$ that are defined by \n",
    "$$\\begin{align*}\n",
    "    & W^1:[x,y,z]=\n",
    "    \\begin{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "            W^1_{1,1} = \\begin{bmatrix} o_1 & \\dots & o_z \\end{bmatrix} \\\\\n",
    "            \\vdots \\\\\n",
    "            W^1_{1,y} = \\begin{bmatrix} p_1 & \\dots & p_z \\end{bmatrix}\n",
    "        \\end{bmatrix}\n",
    "        & \\dots &\n",
    "        \\begin{bmatrix}\n",
    "            W^1_{x, 1} = \\begin{bmatrix} q_1 & \\dots & q_z \\end{bmatrix} \\\\\n",
    "            \\vdots \\\\\n",
    "            W^1_{x, y} = \\begin{bmatrix} r_1 & \\dots & r_z \\end{bmatrix}\n",
    "        \\end{bmatrix}\n",
    "    \\end{bmatrix}, \\\\\n",
    "    & B^1:[z]=\\begin{bmatrix} b_1 & \\dots & b_z \\end{bmatrix}.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the Design\n",
    "These are the results of the design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "These are the conclusions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d821b4a64170991985eb45db6935869b280903e397e6926b4c3c85848ee05baf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
